%matplotlib inline
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
import sys
import datetime
import scipy
import scipy.stats as ss
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
import random
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.ensemble import IsolationForest
np.random.seed(20) # include a seed for reproducibility

# generate the normal data using number of points, 
# number of relevant features, mean and standard deviation
norm_pts = 5000
norm_dim = 5
norm_mean = 0.0
norm_stdev = 0.5
norm_data = np.random.normal(norm_mean, norm_stdev,(norm_pts, norm_dim))

# Note:
#
# Do not confuse the two uses of 'normal'
# The 'normal' in np.random.multivariate_normal refers 
# to a Gaussian distribution and is not related in general 
# to normal vs. anomaly 
# Generate Gaussian distribution for preliminary anomaly data
# using number of points, number of relevant features, 
# mean and standard deviation
anom_pts = 100
anom_dim = 5
anom_mean = 0.0
anom_stdev = 0.5
anom_data_prelim = np.random.normal(anom_mean, anom_stdev,(anom_pts, anom_dim))
# Transform into ring distribution for final anomaly data
def transform_to_ring(data, radius, spread):
    """
    Transforms the data provided into a ring distribution.
    ---
    Inputs: data (the data to be transformed, np.array),
    radius (radius of ring, float)
    spread (spread of data about radius, float)
    Outputs: transformed_data (np.array)
    """
    transformed_data_list = []
    for item in data:
        z = np.array(item) # in case data is provided as list
        transformed_data_list.append(z*spread+ radius*z / np.linalg.norm(z))
        transformed_data = np.array(transformed_data_list)
    return transformed_data

anom_data = transform_to_ring(anom_data_prelim, 2.0, 1.0)
len(anom_data)
fig = plt.figure(figsize=(6,4))
ax1 = fig.add_subplot(111)

ax1.scatter(norm_data[:,0], norm_data[:,1], s=30, c='b', marker="o", 
            label='normal')
ax1.scatter(anom_data[:,0], anom_data[:,1], s=30, c='r', marker="o", 
            label='anomaly')
plt.legend(loc='best');
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.show()
combined_data = np.concatenate((norm_data, anom_data), axis=0)
combined_data.shape
noise_dim = 5
noise_mean = 0
noise_stdev = 4
noise_norm = np.random.normal(noise_mean, noise_stdev,(norm_pts, noise_dim))
noise_anom = np.random.normal(noise_mean, noise_stdev,(anom_pts, noise_dim ))

norm_data_full = np.concatenate((norm_data, noise_norm), axis=1)
anom_data_full = np.concatenate((anom_data, noise_anom), axis=1)
print(norm_data_full.shape)
print(anom_data_full.shape)
all_data = np.concatenate((norm_data_full, anom_data_full), axis=0)
all_data.shape
def feature_bagging(data):
    """
    Selects subspace data using feature bagging.
   
    Args: 
        data: the full data to be sampled (np.array)
        
    Returns: 
        subspace_data: the subspace data (np.array)
    """
    # Find the size of the full dimensional space
    full_dim = data.shape[1]
    # select size of subspace
    size_subspace = np.random.randint(full_dim // 2, full_dim - 1)
    # select features without replacement 
    subspace_index = np.random.choice(full_dim, size_subspace, 
                                      replace=False)
    # select subspace data 
    # Originally, features are data columns
    # Transponse to select features as rows (easier)
    # and then transponse back to restore features as columns
    subspace_data = (data.T[sorted(subspace_index),:]).T
    return subspace_data
def anomaly_subspace_method(data, repeat):
    """
    Carries out anomaly detection using feature bagging.
    Uses Local Outlier Factor (LOF) for anomaly detection.
    Uses cumulative-sum approach to combine anomaly detection scores. 
    from each iteration
    
    Args: 
        data: the full data to be sampled (np.array)
        repeat: number of iterations for feature bagging (int)
    Returns:
        final_scores: the anomaly score of each point in the data (list)
    """
    final_scores = np.zeros(len(data))
    for i in range(repeat):
        subspace_data = feature_bagging(data)
        clf = LocalOutlierFactor(n_neighbors=20, metric='manhattan')
        y_pred = clf.fit_predict(subspace_data)
        X_scores = clf.negative_outlier_factor_
        final_scores += X_scores
    return final_scores
        
cumulative_scores = anomaly_subspace_method(all_data, 10)
print(cumulative_scores[0:20])
print(cumulative_scores[5080:5100])
ranking_by_index = np.argsort(cumulative_scores)
print(ranking_by_index[0:40])
# Create labels based on known data
# We know that the first *norm_pts* are normal data (0)
# and the next *anom_pts* are anomalies (1)
label_true = [0]*norm_pts
print(len(label_true))
for i in range(anom_pts):
        label_true.append(1)
print(len(label_true))
label_pred = [0]*len(all_data)
for index, item in enumerate(ranking_by_index):
    label_pred[item] = 1
    if index == anom_pts:
        # We stop once we have label *anom_pts* points 
        break
label_true_series = pd.Series(label_true, name='True')
label_pred_series = pd.Series(label_pred, name='Predicted')
df_confusion_matrix = pd.crosstab(label_true_series, label_pred_series)
df_confusion_matrix

confusion_matrix(label_true, label_pred)
accuracy_score(label_true, label_pred)
isolation_forest = IsolationForest(n_estimators=100, max_samples=256, 
                                   contamination=0.01961, max_features=10)
isolation_forest.fit(all_data)
anomaly_score = isolation_forest.decision_function(all_data)
print(anomaly_score[0:20])
print(anomaly_score[5040:5100])
pred_forest = isolation_forest.predict(all_data)
print(pred_forest[5000:5100])
label_pred_forest = pred_forest
# normal data label switched from 1 to 0
label_pred_forest [label_pred_forest > 0] = 0 
# anomaly label switched from -1 to 1
label_pred_forest [label_pred_forest < 0] = 1 
accuracy_score(label_true, label_pred_forest)
